{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the datasets\n",
    "lines = open('cornell movie-dialogs corpus/movie_lines.txt', encoding='utf-8', \n",
    "             errors = \"ignore\").read().split(\"\\n\")\n",
    "conversations = open('cornell movie-dialogs corpus/movie_conversations.txt', \n",
    "                     encoding='utf-8', errors = \"ignore\").read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionnary to map each line to its id\n",
    "idToLine = {}\n",
    "for line in lines:\n",
    "    _line = line.split(\" +++$+++ \")\n",
    "\n",
    "    # select only the lines with a conversation exchange length of 5\n",
    "    if len(_line) == 5:\n",
    "        idToLine[_line[0]] = _line[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of the conversations\n",
    "conversations_ids = []\n",
    "for conversation in conversations[:-1]:\n",
    "    #remove the square brackets, quotes and empty spaces\n",
    "    _conversation = conversation.split(\" +++$+++ \")[-1][1:-1].replace(\"'\", \"\").replace(\" \", \"\")\n",
    "    conversations_ids.append(_conversation.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting separately the questions and the answers\n",
    "rawQuestions = []\n",
    "rawAnswers = []\n",
    "\n",
    "for conversation in conversations_ids:\n",
    "    for i in range(len(conversation) - 1):\n",
    "        rawQuestions.append(idToLine[conversation[i]])\n",
    "        rawAnswers.append(idToLine[conversation[i+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the texts\n",
    "def clean_text(text):\n",
    "    \n",
    "    contractions = {\n",
    "    \"ain't\": \"am not / are not\",\n",
    "    \"aren't\": \"are not / am not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he had / he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he shall / he will\",\n",
    "    \"he'll've\": \"he shall have / he will have\",\n",
    "    \"he's\": \"he has / he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how has / how is\",\n",
    "    \"i'd\": \"I had / I would\",\n",
    "    \"i'd've\": \"I would have\",\n",
    "    \"i'll\": \"I shall / I will\",\n",
    "    \"i'll've\": \"I shall have / I will have\",\n",
    "    \"i'm\": \"I am\",\n",
    "    \"i've\": \"I have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it had / it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it shall / it will\",\n",
    "    \"it'll've\": \"it shall have / it will have\",\n",
    "    \"it's\": \"it has / it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she had / she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she shall / she will\",\n",
    "    \"she'll've\": \"she shall have / she will have\",\n",
    "    \"she's\": \"she has / she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as / so is\",\n",
    "    \"that'd\": \"that would / that had\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that has / that is\",\n",
    "    \"there'd\": \"there had / there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there has / there is\",\n",
    "    \"they'd\": \"they had / they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they shall / they will\",\n",
    "    \"they'll've\": \"they shall have / they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we had / we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what shall / what will\",\n",
    "    \"what'll've\": \"what shall have / what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what has / what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when has / when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where has / where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who shall / who will\",\n",
    "    \"who'll've\": \"who shall have / who will have\",\n",
    "    \"who's\": \"who has / who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why has / why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you had / you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you shall / you will\",\n",
    "    \"you'll've\": \"you shall have / you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\"\n",
    "    }\n",
    "    \n",
    "    text = text.lower()\n",
    "    for word in text.split():\n",
    "        if word in contractions:\n",
    "            text = text.replace(word, contractions[word])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for question in rawQuestions:\n",
    "    questions.append(clean_text(question))\n",
    "    \n",
    "for answer in rawAnswers:\n",
    "    answers.append(clean_text(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out the questions and answers that are too short or too long\n",
    "short_questions = []\n",
    "short_answers = []\n",
    "i = 0\n",
    "for question in questions:\n",
    "    if 2 <= len(question.split()) <= 25:\n",
    "        short_questions.append(question)\n",
    "        short_answers.append(answers[i])\n",
    "    i += 1\n",
    "questions = []\n",
    "answers = []\n",
    "i = 0\n",
    "for answer in short_answers:\n",
    "    if 2 <= len(answer.split()) <= 25:\n",
    "        answers.append(answer)\n",
    "        questions.append(short_questions[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['can we make this quick?  roxanne korrine and andrew barrett are having an incredibly horrendous public break- up on the quad.  again.', 'well, i thought we had / we would start with pronunciation, if that has / that is okay with you.', 'not the hacking and gagging and spitting part.  please.']\n",
      "['well, i thought we had / we would start with pronunciation, if that has / that is okay with you.', 'not the hacking and gagging and spitting part.  please.', \"okay... then how 'bout we try out some french cuisine.  saturday?  night?\"]\n"
     ]
    }
   ],
   "source": [
    "print(questions[:3])\n",
    "print(answers[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the words that have a low amount of occurrences (optimised for training)\n",
    "word2count = {}\n",
    "for question in questions:\n",
    "    for word in question.split():\n",
    "        if word not in word2count:\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for answer in answers:\n",
    "    for word in answer.split():\n",
    "        if word not in word2count:\n",
    "            word2count[word] = 1\n",
    "        else:\n",
    "            word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionnaries to map the words in questions and answers to a unique id\n",
    "\n",
    "# remove the least common words, in this case less than 20 times\n",
    "threshold = 20\n",
    "\n",
    "questionswords2int = {}\n",
    "word_number = 0\n",
    "for word, count in word2count.items():\n",
    "    if count >= threshold:\n",
    "        questionswords2int[word] = word_number\n",
    "        word_number += 1\n",
    "        \n",
    "answerswords2int = {}\n",
    "word_number = 0\n",
    "for word, count in word2count.items():\n",
    "    if count >= threshold:\n",
    "        answerswords2int[word] = word_number\n",
    "        word_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the last tokens to these two dictionnaries\n",
    "tokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']\n",
    "for token in tokens:\n",
    "    questionswords2int[token] = len(questionswords2int) + 1\n",
    "    answerswords2int[token] = len(answerswords2int) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the inverse dictionary of the answerswords2int dictionnary\n",
    "answersint2word = {w_i: w for w, w_i in answerswords2int.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add EOS token to the end of every answer (required for decoding layers in seq2seq model)\n",
    "for i in range(len(answers)):\n",
    "    answers[i] += ' <EOS>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate all the questions and answers into integers and replace all the words that were filtered out by <OUT>\n",
    "questions_to_int = []\n",
    "for question in questions:\n",
    "    ints = []\n",
    "    for word in question.split():\n",
    "        if word not in questionswords2int:\n",
    "            ints.append(questionswords2int[\"<OUT>\"])\n",
    "        else:\n",
    "            ints.append(questionswords2int[word])\n",
    "    \n",
    "    questions_to_int.append(ints)\n",
    "    \n",
    "answers_to_int = []\n",
    "for answer in answers:\n",
    "    ints = []\n",
    "    for word in answer.split():\n",
    "        if word not in answerswords2int:\n",
    "            ints.append(answerswords2int[\"<OUT>\"])\n",
    "        else:\n",
    "            ints.append(answerswords2int[word])\n",
    "    \n",
    "    answers_to_int.append(ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the questions and answers by the length of the questions (speeds up the training by reducing padding)\n",
    "sorted_questions = []\n",
    "sorted_answers = []\n",
    "\n",
    "for questLen in range(1, 25 + 1):\n",
    "    for idx, quest in enumerate(questions_to_int):\n",
    "        if len(quest) == questLen:\n",
    "            sorted_questions.append(quest)\n",
    "            sorted_answers.append(answers_to_int[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow 1: building seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create placeholders for the inputs and targets of the model\n",
    "def model_inputs():\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name = 'input')\n",
    "    targets= tf.placeholder(tf.int32, [None, None], name = 'target')\n",
    "    lr = tf.placeholder(tf.float32, name = 'learning rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')\n",
    "    return inputs, tagerts, lr, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the targets\n",
    "def preprocess_targets(targets, word2int, batch_size):\n",
    "    '''\n",
    "    Add SOS to the start of each liine by creating a tensor with a scalar value\n",
    "    and concatenating it with the tensors of the targets\n",
    "    '''\n",
    "    left_side = tf.fill([batch_size, 1], word2int[\"<SOS>\"])\n",
    "    right_side = tf.strided_slice(targets, [0,0], [batch_size,-1], [1,1])\n",
    "    preprocessed_targets = tf.concat([left_side, right_side], axis = 1)\n",
    "    return preprocessed_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the encoder RNN layer\n",
    "def encoder_rnn(rnn_inputs, rnn_size, num_layers, keep_prob, seq_length):\n",
    "    # define lstm layer and add dropout layer\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, \n",
    "                                                 input_keep_prob = keep_prob)\n",
    "    \n",
    "    #take a list of RNNCells and wrap them into a single cell\n",
    "    encoder_cell = tf.contrib.runn.MultiRNNCell([lstm_dropout] * num_layers)\n",
    "    \n",
    "    # pass through bidirectionnal rnn to obtain state\n",
    "    encoder_output, encoder_state = tf.nn.bidirectional_dynamic_rnn(cell_fw = encoder_cell, \n",
    "                                                       cell_bw = encoder_cell,\n",
    "                                                      sequence_length = seq_length,\n",
    "                                                      inputs = rnn_inputs,\n",
    "                                                      dtype = tf.float32)\n",
    "    \n",
    "    return encoder_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoding the training set\n",
    "def decode_training_set(encoder_state, decoder_cell, decoder_embedded_input, \n",
    "                        sequence_length, decoding_scope, output_function, \n",
    "                        keep_prob, batch_size):\n",
    "    # determine the states to be backpropagated during the training of the model\n",
    "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size])\n",
    "    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = 'bahdanau', num_units = decoder_cell.output_size)\n",
    "    \n",
    "    training_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_train(encoder_state[0], attention_keys, attention_values, attention_score_function, attention_construct_function, name = \"attn_dec_train\")\n",
    "    decoder_output, decoder_final_state, decoder_final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell, training_decoder_function, decoder_embedded_input, sequence_length, scope = decoding_scope)\n",
    "    \n",
    "    decoder_output_dropout = tf.nn.dropout(decoder_output, keep_prob)\n",
    "    return output_function(decoder_output_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoding the test/validation set\n",
    "def decode_test_set(encoder_state, decoder_cell, decoder_embeddings_matrix, sos_id, eos_id, max_length, num_words, \n",
    "                    sequence_length, decoding_scope, output_function, keep_prob, batch_size):\n",
    "    \n",
    "    attention_states = tf.zeros([batch_size, 1, decoder_cell.output_size])\n",
    "    attention_keys, attention_values, attention_score_function, attention_construct_function = tf.contrib.seq2seq.prepare_attention(attention_states, attention_option = 'bahdanau', num_units = decoder_cell.output_size)\n",
    "    test_decoder_function = tf.contrib.seq2seq.attention_decoder_fn_inference(output_function, encoder_state[0], attention_keys, attention_values, attention_score_function, attention_construct_function, decoder_embeddings_matrix, sos_id, eos_id, max_length, num_words, name = \"attn_dec_inf\")\n",
    "    \n",
    "    test_predictions, decoder_final_state, decoder_final_context_state = tf.contrib.seq2seq.dynamic_rnn_decoder(decoder_cell, test_decoder_function, scope = decoding_scope)\n",
    "    \n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the decoder rnn\n",
    "def decoder_rnn(decoder_embedded_input, decoder_embeddings_matrix, \n",
    "                encoder_state, num_words, sequence_length, rnn_size, \n",
    "                num_layers, word2int, keep_prob, batch_size):\n",
    "    with tf.variable_scope(\"decoding\") as decoding_scope:\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell()\n",
    "        lstm_dropout = tf.contrib.rnn.DropoutWrapper(lstm, input_keep_prob=keep_prob)\n",
    "        \n",
    "        #take a list of RNNCells and wrap them into a single cell\n",
    "        decoder_cell = tf.contrib.runn.MultiRNNCell([lstm_dropout] * num_layers)\n",
    "        \n",
    "        # generates normal truncated distribution of the weights\n",
    "        weights = tf.truncated_normal_initializer(stddev=0.1)\n",
    "        biases = tf.zeros_initializer()\n",
    "        \n",
    "        output_function = lambda x: tf.contrib.layers.fully_connected(x, num_words, None, scope = decoding_scope, weights_initializers=weights, biases_initializer=biases)\n",
    "        \n",
    "        training_predictions = decode_training_set(encoder_state, decoder_cell, decoder_embedded_input, sequence_length, decoding_scope, output_function, keep_prob, batch_size)\n",
    "        \n",
    "        decoding_scope.reuse_variables()\n",
    "        test_predcitions = decode_test_set(encoder_state, decoder_cell, decoder_embeddings_matrix, word2int['<SOS>'], word2int['<EOS>'], sequence_length -1, num_words, decoding_scope, output_function, keep_prob, batch_size)\n",
    "    \n",
    "    return training_predictions, test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the seq2seq model\n",
    "def seq2seq_model(inputs, targets, keep_prob, batch_size, seq_length, answers_num_words, \n",
    "                  questions_num_words, encoder_embedding_size, decoder_embedding_size, rnn_size, \n",
    "                  num_layers, questionswords2int):\n",
    "    \n",
    "    encoder_embedded_input = tf.contrib.layers.embed_sequence(inputs, answers_num_words+1, \n",
    "                                                              encoder_embedding_size, \n",
    "                                                              initializer=tf.random_uniform_initializer(0,1))\n",
    "    encdoer_state = encoder_rnn(encoder_embedded_input, rnn_size, num_layers, keep_prob, sequence_length)\n",
    "    \n",
    "    # preprocess the targets\n",
    "    preprocessed_targets = preprocess_targets(targets, questionswords2int, batch_size)\n",
    "    \n",
    "    # initialize the embeddings matrix of the decoder with random numbers between 0 an 1\n",
    "    decoder_embeddings_matrix = tf.Variabletf.random_uniform([questions_num_words + 1, decoder_embedding_size], 0, 1)\n",
    "    \n",
    "    # get the embedded inputs\n",
    "    decoder_embedded_input = tf.nn.embedding_lookup(decoder_embeddings_matrix, preprocessed_targets)\n",
    "    \n",
    "    training_predictions, test_predictions = decoder_rnn(decoder_embedded_input, decoder_embeddings_matrix, encoder_state, \n",
    "                                                         questions_num_words, sequence_length, rnn_size, num_layers, \n",
    "                                                         questionswords2int, keep_prob, batch_size)\n",
    "    return training_predictions, test_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the hyperparameters\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "rnn_size = 512\n",
    "num_layers = 3\n",
    "encoding_embedding_size = 512\n",
    "decoding_embedding_size = 512\n",
    "learning_rate = 0.01\n",
    "learning_rate_decay = 0.9\n",
    "min_learning_rate = 0.0001\n",
    "\n",
    "# dropout rate of 50% for hidden units\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'reset_default_graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-e287324d34dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# define session\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minteractiveSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'reset_default_graph'"
     ]
    }
   ],
   "source": [
    "# define session\n",
    "tf.reset_default_graph()\n",
    "session = tf.interactiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model inputs\n",
    "inputs, targets, lr, keep_prob = model_inputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the sequence length\n",
    "max_length = 25\n",
    "sequence_length = tf.placeholder_with_default(max_length, None, name = 'sequence_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the shape of the inputs tensor\n",
    "input_shape = tf.shape(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the training and test predictions\n",
    "training_predictions, test_predictions = seq2seq_model(tf.reverse(inputs, [-1]), targets, keep_prob, batch_size, sequence_length, \n",
    "                                                       len(answerswords2int), len(questionswords2int), encoding_embedding_size,\n",
    "                                                       decdoding_embedding_size, rnn_size, num_layers, questionswords2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the loss error, the optimizer and gradient clipping\n",
    "with tf.name_scope('optimization'):\n",
    "    loss_error = tf.contrib.seq2seq.sequence_loss(training_predictions, targets, tf.ones([input_shape[0], sequence_length]))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    gradients = optimizer.compute_gradients(loss_error)\n",
    "    clipped_gradients = [(tf.clip_by_value(grad_tensor, -5., 5.), grad_variable) for grad_tensor, grad_variable in gradients if grad_tensor is not None]\n",
    "    optimizer_gradient_clipping = optimizer.apply_gradients(clipped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding the sequences with the <PAD> token so that question length = answer length\n",
    "def apply_padding(batch_of_sequences, word2int):\n",
    "    max_sequence_length = max([len(sequence) for sequence in batch_of_sequences])\n",
    "    return [sequence + [word2int[\"<PAD>\"]]*(max_sequence_length-len(sequence)) for sequence in batch_of_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into batches of questions and answers\n",
    "def split_into_batches(questions, answers, batch_size):\n",
    "    for batch_index in range(0, len(questions)//batch_size):\n",
    "        start_index = batch_index * batch_size\n",
    "        questions_in_batch = questions[start_index : start_index+batch_size]\n",
    "        answers_in_batch = answers[start_index : start_index+batch_size]\n",
    "        padded_questions_in_batch = np.array(apply_padding(questions_in_batch, questionswords2int))\n",
    "        padded_answers_in_batch = np.array(apply_padding(answers_in_batch, answerswords2int))\n",
    "        yield padded_questions_in_batch, padded_answers_in_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the questions and answers into training and validation sets\n",
    "training_validation_split = int(len(sorted_questions)*0.15)\n",
    "training_questions = sorted_questions[training_validation_split:]\n",
    "training_answers = sorted_answers[training_validation_split:]\n",
    "validation_questions = sorted_questions[:training_validation_split]\n",
    "validation_answers = sorted_answers[:training_validation_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "\n",
    "# check the training loss every batch_index_check_training_loss and the \n",
    "# validation loss half-way through the epoch and at its end\n",
    "batch_index_check_training_loss = 100\n",
    "batch_index_check_validation_loss = (len(training_questions) // batch_size // 2) - 1 \n",
    "total_training_loss_error = 0\n",
    "\n",
    "# elements for early stopping\n",
    "list_validation_loss_error = []\n",
    "early_stopping_check = 0\n",
    "early_stopping_stop = 100\n",
    "checkpoint = \"chatbot_weights.ckpt\"\n",
    "\n",
    "# initilize global variables\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    for batch_index, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(training_questions, training_answers, batch_size)):\n",
    "        starting_time = time.time()\n",
    "        _, batch_training_loss_error = session.run([optimizer_gradient_clipping, loss_error], {inputs: padded_questions_in_batch, targets: padded_answers_in_batch, lr: learning_rate, sequence_length: padded_answers_in_batch.shape[1], keep_prob: keep_probability})\n",
    "        total_training_loss_error += batch_training_loss_error\n",
    "        ending_time = time.time()\n",
    "        batch_time = ending_time - starting_time\n",
    "        \n",
    "        if batch_index % batch_index_check_training_loss ==0:\n",
    "            print(\"Epoch: {:>3}/{}, Batch:{:>4}, Training Loss Error: {:>6.3f}, Training Time on 100 Batches: {:d} seconds\".format(epoch, epochs, batch_index, len(training_questions)//batch_size, total_training_loss_error/batch_index_check_training_loss, int(batch_time*batch_index_check_training_loss)))\n",
    "            total_training_loss_error = 0\n",
    "            \n",
    "        if batch_index % batch_index_check_validation_loss == 0 and batch_index > 0:\n",
    "            total_validation_loss_error = 0\n",
    "            starting_time = time.time()\n",
    "            for batch_index_validation, (padded_questions_in_batch, padded_answers_in_batch) in enumerate(split_into_batches(validation_questions, validation_answers, batch_size)):\n",
    "                # reminder: optimizer and keep_prob are only used in training\n",
    "                batch_validation_loss_error = session.run(loss_error, {inputs: padded_questions_in_batch, targets: padded_answers_in_batch, lr: learning_rate, sequence_length: padded_answers_in_batch.shape[1], keep_prob: 1})\n",
    "                total_validation_loss_error += batch_validation_loss_error\n",
    "            ending_time = time.time()\n",
    "            batch_time = ending_time - starting_time\n",
    "            average_validation_loss_error = total_validation_loss_error / (len(validation_questions) / batch_size)\n",
    "            print(\"Validation Loss Error: {:>6.3f}, Batch Validation Time: {:d} seconds\".format(average_validation_loss_error, int(batch_time)))\n",
    "            \n",
    "            # apply decay to learning rate\n",
    "            learning_rate *= learning_rate_decay\n",
    "            if learning_rate < min_learning_rate:\n",
    "                learning_rate = min_learning_rate\n",
    "            \n",
    "            # early stopping\n",
    "            list_validation_loss_error.append(average_validation_loss_error)\n",
    "            if average_validation_loss_error <= min(list_validation_loss_error):\n",
    "                early_stopping_check = 0\n",
    "                saver = tf.train.Saver()\n",
    "                saver.save(session, checkpoint)\n",
    "            else:\n",
    "                early_stopping_check += 1\n",
    "                if early_stopping_check == early_stopping_stop:\n",
    "                    break\n",
    "    \n",
    "    if early_stopping_check == early_stopping_stop:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the weights and running the session\n",
    "checkpoint = './chatbot_weights.ckpt'\n",
    "session = tf.InteractiveSession()\n",
    "session.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(session, checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the questions from strings to lists of encoding integers\n",
    "def convert_string2int(question, word2int):\n",
    "    question = clean_text(question)\n",
    "    \n",
    "    # get the int value representing the word in our dictionnary or the int for <OUT> \n",
    "    # if the word does not exist in the dictionnary\n",
    "    return [word2int.get(word, word2int['<OUT>']) for word in question.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the chat\n",
    "while(True):\n",
    "    question = input('You: ')\n",
    "    if question == 'Goodbye':\n",
    "        break\n",
    "    question = convert_string2int(question, questionswords2int)\n",
    "    question = question + [questionswords2int[\"<PAD>\"]] * (max_length - len(questions))\n",
    "    fake_batch = np.zeros((batch_size, max_length))\n",
    "    fake_batch[0] = question\n",
    "    predicted_answer = session.run(test_predictions, {inputs: fake_batch, keep_prob: 0.5})[0]\n",
    "    \n",
    "    answer = ''\n",
    "    for word_int in np.argmax(predicted_answer, 1):\n",
    "        if answersints2word[word_int] == 'i':\n",
    "            token = 'I'\n",
    "        elif answersints2word[word_int] == '<EOS>':\n",
    "            token = '.'\n",
    "        elif answersints2word[word_int] == '<OUT>':\n",
    "            token = '.'\n",
    "        else:\n",
    "            token = ' ' + answersints2word[word_int]\n",
    "        \n",
    "        answer += token\n",
    "        if token == '.':\n",
    "            break\n",
    "    print('ChatBot: ' + answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
