{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import preprocessing\n",
    "from seq2seqModel import Encoder, Decoder, BahdanauAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the datasets\n",
    "lines = open('cornell movie-dialogs corpus/movie_lines.txt', encoding='utf-8', \n",
    "             errors = \"ignore\").read().split(\"\\n\")\n",
    "conversations = open('cornell movie-dialogs corpus/movie_conversations.txt', \n",
    "                     encoding='utf-8', errors = \"ignore\").read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary that maps each line and its id\n",
    "id2line = {}\n",
    "for line in lines:\n",
    "    _line = line.split(' +++$+++ ')\n",
    "    if len(_line) == 5:\n",
    "        id2line[_line[0]] = _line[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of all of the conversations\n",
    "conversations_ids = []\n",
    "for conversation in conversations[:-1]:\n",
    "    _conversation = conversation.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \"\").replace(\" \", \"\")\n",
    "    conversations_ids.append(_conversation.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting separately the questions and the answers\n",
    "rawQuestions = []\n",
    "rawAnswers = []\n",
    "\n",
    "for conversation in conversations_ids:\n",
    "    for i in range(len(conversation) - 1):\n",
    "        rawQuestions.append(id2line[conversation[i]])\n",
    "        rawAnswers.append(id2line[conversation[i+1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for question in rawQuestions:\n",
    "    questions.append(preprocessing.clean_text(question))\n",
    "    \n",
    "for answer in rawAnswers:\n",
    "    answers.append(preprocessing.clean_text(answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out the questions and answers that are too short or too long\n",
    "short_questions = []\n",
    "short_answers = []\n",
    "i = 0\n",
    "for question in questions:\n",
    "    if 2 <= len(question.split()) <= 25:\n",
    "        short_questions.append(question)\n",
    "        short_answers.append(answers[i])\n",
    "    i += 1\n",
    "questions = []\n",
    "answers = []\n",
    "i = 0\n",
    "for answer in short_answers:\n",
    "    if 2 <= len(answer.split()) <= 25:\n",
    "        answers.append(answer)\n",
    "        questions.append(short_questions[i])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the questions and answers by the length of the questions (speeds up the training by reducing padding)\n",
    "sorted_questions = []\n",
    "sorted_answers = []\n",
    "\n",
    "for questLen in range(1, 25 + 1):\n",
    "    for idx, quest in enumerate(questions):\n",
    "        if len(quest) == questLen:\n",
    "            sorted_questions.append(preprocessing.preprocess_sentence(questions[idx]))\n",
    "            sorted_answers.append(preprocessing.preprocess_sentence(answers[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 1000\n",
    "\n",
    "input_tensor, target_tensor, tokenizer = preprocessing.tokenize(sorted_questions[:num_examples], sorted_answers[:num_examples])\n",
    "max_input_length, max_target_length = input_tensor.shape[1], input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the hyperparameters\n",
    "batch_size = 64\n",
    "lstm_units = 512\n",
    "num_layers = 3\n",
    "encoder_embedding_size = 512\n",
    "decoder_embedding_size = 512\n",
    "steps_per_epoch = len(input_tensor_train)//batch_size\n",
    "\n",
    "vocab_size = len(tokenizer.word_index)+1\n",
    "\n",
    "# dropout rate of 50% for hidden units\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(len(input_tensor_train))\n",
    "dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "dataset_val = tf.data.Dataset.from_tensor_slices((input_tensor_val, target_tensor_val)).shuffle(len(input_tensor_val))\n",
    "dataset_val = dataset_val.batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = input_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.01\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=100,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred, input_shape, sequence_length=25):\n",
    "    #https://www.tensorflow.org/addons/api_docs/python/tfa/seq2seq/sequence_loss\n",
    "    #return tfa.seq2seq.sequence_loss(pred, real, tf.ones([input_shape[0], sequence_length]))\n",
    "    \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_size, encoder_embedding_size, lstm_units, batch_size, keep_probability)\n",
    "decoder = Decoder(vocab_size, decoder_embedding_size, lstm_units, batch_size)\n",
    "\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inputs, targets, encoder_hidden):\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        encoder_output, encoder_hidden = encoder(inputs, encoder_hidden)\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoder_input = tf.expand_dims([tokenizer.word_index['<start>']] * batch_size, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targets.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "            \n",
    "            #print(targets[:, t].shape, predictions.shape, input_shape)\n",
    "            loss += loss_function(targets[:, t], predictions, input_shape)\n",
    "            \n",
    "            # using teacher forcing\n",
    "            decoder_input = tf.expand_dims(targets[:, t], 1)\n",
    "            \n",
    "    batch_loss = (loss / int(targets.shape[1]))\n",
    "        \n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inputs, targets, encoder_hidden):\n",
    "    \n",
    "    loss = 0\n",
    "    encoder_output, encoder_hidden = encoder(inputs, encoder_hidden)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    decoder_input = tf.expand_dims([tokenizer.word_index['<start>']] * batch_size, 1)\n",
    "    \n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    for t in range(1, targets.shape[1]):\n",
    "        # passing enc_output to the decoder\n",
    "        predictions, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "        loss += loss_function(targets[:, t], predictions, input_shape)\n",
    "        \n",
    "        # using teacher forcing\n",
    "        decoder_input = tf.expand_dims(targets[:, t], 1)\n",
    "    \n",
    "    batch_loss = (loss / int(targets.shape[1]))\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 0, Training Loss 2.395, Training Time on 10 batches: 150.50 s\n",
      "Avg Validation Loss Error:  2.073, Validation Time: 7.64 s\n",
      "Epoch 1, Batch 10, Training Loss 2.251, Training Time on 10 batches: 3.69 s\n",
      "Avg Validation Loss Error:  2.453, Validation Time: 7.75 s\n",
      "Epoch 1, Training Loss 2.185, Avg Validation Loss 2.453\n",
      "\n",
      "Epoch 2, Batch 0, Training Loss 2.873, Training Time on 10 batches: 3.17 s\n",
      "Avg Validation Loss Error:  1.946, Validation Time: 7.27 s\n",
      "Epoch 2, Batch 10, Training Loss 1.864, Training Time on 10 batches: 4.18 s\n",
      "Avg Validation Loss Error:  1.889, Validation Time: 7.53 s\n",
      "Epoch 2, Training Loss 2.042, Avg Validation Loss 1.889\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 2\n",
    "batch_training_loss_check = 10\n",
    "batch_validation_loss_check = steps_per_epoch // 2 - 1\n",
    "\n",
    "# elements for early stopping\n",
    "list_validation_loss_error = []\n",
    "early_stopping_check = 0\n",
    "early_stopping_stop = 100\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_train_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        start = time.time()\n",
    "        batch_train_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_train_loss += batch_train_loss\n",
    "        end = time.time()\n",
    "        batch_time = end - start\n",
    "        \n",
    "        if batch % batch_training_loss_check == 0:\n",
    "            print('Epoch {}, Batch {}, Training Loss {:.3f}, Training Time on {} batches: {:.2f} s'.format(epoch + 1,\n",
    "                                                   batch, batch_train_loss.numpy(), batch_training_loss_check, \n",
    "                                                   batch_time))\n",
    "        \n",
    "        if batch % batch_validation_loss_check == 0 and batch > 0:\n",
    "            total_valid_loss = 0\n",
    "            start_val_time = time.time()\n",
    "            \n",
    "            for (batch_val, (inp_val, targ_val)) in enumerate(dataset_val.take(len(input_tensor_val)//batch_size)):\n",
    "                total_valid_loss += evaluate(inp_val, targ_val, enc_hidden)\n",
    "                \n",
    "            ending_val_time = time.time()\n",
    "            val_time = ending_val_time - start_val_time\n",
    "            \n",
    "            average_validation_loss = total_valid_loss / (len(input_tensor_val) / batch_size)\n",
    "            print(\"Avg Validation Loss Error: {:>6.3f}, Validation Time: {:.2f} s\".format(average_validation_loss, val_time))\n",
    "            \n",
    "            # early stopping\n",
    "            list_validation_loss_error.append(average_validation_loss)\n",
    "            \n",
    "            if average_validation_loss <= min(list_validation_loss_error):\n",
    "                early_stopping_check = 0\n",
    "                checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "            else:\n",
    "                early_stopping_check += 1\n",
    "                \n",
    "            if early_stopping_check == early_stopping_stop:\n",
    "                break\n",
    "                \n",
    "    print('Epoch {}, Training Loss {:.3f}, Avg Validation Loss {:.3f}\\n'.format(epoch + 1, \n",
    "                                                                                total_train_loss / steps_per_epoch, \n",
    "                                                                                average_validation_loss))\n",
    "    \n",
    "    if early_stopping_check == early_stopping_stop:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
